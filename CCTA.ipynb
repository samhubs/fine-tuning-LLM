{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "class CCTAVisualDataset(Dataset):\n",
    "    def __init__(self, size=64, num_samples=100):\n",
    "        self.size = size\n",
    "        self.num_samples = num_samples\n",
    "        self.samples = []\n",
    "        \n",
    "        print(\"Generating synthetic dataset...\")\n",
    "        for _ in tqdm(range(num_samples)):\n",
    "            self.samples.append(self.generate_sample())\n",
    "\n",
    "    def centerline_to_volume(self, centerline):\n",
    "        \"\"\"Convert centerline points to volume representation\"\"\"\n",
    "        volume = np.zeros((self.size, self.size, self.size))\n",
    "        centerline_rounded = np.round(centerline).astype(int)\n",
    "        # Clip to ensure within bounds\n",
    "        centerline_rounded = np.clip(centerline_rounded, 0, self.size-1)\n",
    "        volume[centerline_rounded[:,0], centerline_rounded[:,1], centerline_rounded[:,2]] = 1\n",
    "        return volume\n",
    "    \n",
    "    def generate_curved_centerline(self):\n",
    "        \"\"\"Generate a curved centerline path\"\"\"\n",
    "        t = np.linspace(0, 2*np.pi, self.size)\n",
    "        x = self.size/2 + self.size/4 * np.cos(t)\n",
    "        y = np.linspace(0, self.size-1, self.size)\n",
    "        z = self.size/2 + self.size/4 * np.sin(t)\n",
    "        return np.stack([x, y, z], axis=1)\n",
    "\n",
    "    def generate_vessel_from_centerline(self, centerline, radius=2):\n",
    "        \"\"\"Generate a vessel volume from centerline\"\"\"\n",
    "        volume = np.zeros((self.size, self.size, self.size))\n",
    "        x, y, z = np.meshgrid(np.arange(self.size),\n",
    "                             np.arange(self.size),\n",
    "                             np.arange(self.size),\n",
    "                             indexing='ij')\n",
    "        coords = np.stack([x, y, z], axis=-1)\n",
    "        \n",
    "        for point in centerline:\n",
    "            dist = np.sqrt(np.sum((coords - point)**2, axis=-1))\n",
    "            volume[dist < radius] = 1\n",
    "        return volume\n",
    "\n",
    "    def generate_sample(self):\n",
    "        \"\"\"Generate one CCTA-like sample with vessel and centerline\"\"\"\n",
    "        # Generate centerline\n",
    "        centerline = self.generate_curved_centerline()\n",
    "        \n",
    "        # Generate vessel\n",
    "        vessel = self.generate_vessel_from_centerline(centerline)\n",
    "        \n",
    "        # Convert to tensor for Gaussian filtering\n",
    "        vessel_tensor = torch.FloatTensor(vessel)\n",
    "        noise = torch.randn_like(vessel_tensor) * 0.1\n",
    "        vessel_tensor = vessel_tensor + noise\n",
    "        \n",
    "        # Apply Gaussian filtering\n",
    "        vessel_filtered = gaussian_filter(vessel_tensor, sigma=0.5)\n",
    "        \n",
    "        # Generate quality score (synthetic)\n",
    "        quality_score = np.random.uniform(0.5, 1.0)\n",
    "        \n",
    "        # Generate plaque (synthetic)\n",
    "        plaque_mask = np.zeros_like(vessel)\n",
    "        plaque_mask[vessel > 0.5] = np.random.randint(0, 4, size=np.sum(vessel > 0.5))\n",
    "        \n",
    "        return {\n",
    "            'image': torch.Tensor(vessel_filtered).unsqueeze(0),\n",
    "            'centerline': torch.FloatTensor(self.centerline_to_volume(centerline)).unsqueeze(0),\n",
    "            'quality_score': torch.FloatTensor([quality_score]),\n",
    "            'plaque_mask': torch.LongTensor(plaque_mask)\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19_IQA(nn.Module):\n",
    "    def __init__(self, input_channels=1):\n",
    "        super(VGG19_IQA, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16 * 16, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet3D, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec3 = self.conv_block(256, 128)\n",
    "        self.dec2 = self.conv_block(128, 64)\n",
    "        self.dec1 = self.conv_block(64, out_channels)\n",
    "        \n",
    "        self.final = nn.Conv3d(out_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool3d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool3d(e2, 2))\n",
    "        \n",
    "        # Decoder\n",
    "        d3 = self.dec3(e3)\n",
    "        d2 = self.dec2(F.interpolate(d3, size=e2.shape[2:]))\n",
    "        d1 = self.dec1(F.interpolate(d2, size=e1.shape[2:]))\n",
    "        \n",
    "        return torch.sigmoid(self.final(d1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCTAAnalysisPipeline:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.quality_model = VGG19_IQA().to(device)\n",
    "        self.segmentation_model = UNet3D().to(device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.quality_optimizer = torch.optim.Adam(self.quality_model.parameters())\n",
    "        self.seg_optimizer = torch.optim.Adam(self.segmentation_model.parameters())\n",
    "        \n",
    "        # Loss functions\n",
    "        self.quality_criterion = nn.MSELoss()\n",
    "        self.seg_criterion = nn.BCELoss()\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        self.quality_model.train()\n",
    "        self.segmentation_model.train()\n",
    "        \n",
    "        total_quality_loss = 0\n",
    "        total_seg_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f'Epoch {epoch}'):\n",
    "            # Move data to device\n",
    "            images = batch['image'].to(self.device)\n",
    "            centerlines = batch['centerline'].to(self.device)\n",
    "            quality_scores = batch['quality_score'].to(self.device)\n",
    "            \n",
    "            # Train quality assessment\n",
    "            self.quality_optimizer.zero_grad()\n",
    "            quality_pred = self.quality_model(images)\n",
    "            quality_loss = self.quality_criterion(quality_pred, quality_scores)\n",
    "            quality_loss.backward()\n",
    "            self.quality_optimizer.step()\n",
    "            \n",
    "            # Train segmentation\n",
    "            self.seg_optimizer.zero_grad()\n",
    "            seg_pred = self.segmentation_model(images)\n",
    "            seg_loss = self.seg_criterion(seg_pred, centerlines)\n",
    "            seg_loss.backward()\n",
    "            self.seg_optimizer.step()\n",
    "            \n",
    "            total_quality_loss += quality_loss.item()\n",
    "            total_seg_loss += seg_loss.item()\n",
    "            \n",
    "        avg_quality_loss = total_quality_loss / len(dataloader)\n",
    "        avg_seg_loss = total_seg_loss / len(dataloader)\n",
    "        \n",
    "        return avg_quality_loss, avg_seg_loss\n",
    "    \n",
    "    def validate(self, dataloader):\n",
    "        self.quality_model.eval()\n",
    "        self.segmentation_model.eval()\n",
    "        \n",
    "        total_quality_loss = 0\n",
    "        total_seg_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                centerlines = batch['centerline'].to(self.device)\n",
    "                quality_scores = batch['quality_score'].to(self.device)\n",
    "                \n",
    "                quality_pred = self.quality_model(images)\n",
    "                seg_pred = self.segmentation_model(images)\n",
    "                \n",
    "                quality_loss = self.quality_criterion(quality_pred, quality_scores)\n",
    "                seg_loss = self.seg_criterion(seg_pred, centerlines)\n",
    "                \n",
    "                total_quality_loss += quality_loss.item()\n",
    "                total_seg_loss += seg_loss.item()\n",
    "        \n",
    "        avg_quality_loss = total_quality_loss / len(dataloader)\n",
    "        avg_seg_loss = total_seg_loss / len(dataloader)\n",
    "        \n",
    "        return avg_quality_loss, avg_seg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(sample, seg_pred):\n",
    "    \"\"\"Visualize original image, true centerline, and predicted centerline\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image (middle slice)\n",
    "    axes[0].imshow(sample['image'][0, 0, :, :, sample['image'].shape[3]//2].cpu())\n",
    "    axes[0].set_title('Original Image')\n",
    "    \n",
    "    # True centerline\n",
    "    axes[1].imshow(sample['centerline'][0, 0, :, :, sample['centerline'].shape[3]//2].cpu())\n",
    "    axes[1].set_title('True Centerline')\n",
    "    \n",
    "    # Predicted centerline\n",
    "    axes[2].imshow(seg_pred[0, 0, :, :, seg_pred.shape[3]//2].cpu().detach())\n",
    "    axes[2].set_title('Predicted Centerline')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.43it/s]\n",
      "Epoch 0:   5%|▌         | 1/20 [00:54<17:20, 54.76s/it]"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = CCTAVisualDataset(size=64, num_samples=100)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = CCTAAnalysisPipeline()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_quality_loss, train_seg_loss = pipeline.train_epoch(train_loader, epoch)\n",
    "    val_quality_loss, val_seg_loss = pipeline.validate(val_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train - Quality Loss: {train_quality_loss:.4f}, Seg Loss: {train_seg_loss:.4f}')\n",
    "    print(f'Val - Quality Loss: {val_quality_loss:.4f}, Seg Loss: {val_seg_loss:.4f}')\n",
    "    \n",
    "    # Visualize results for first validation sample\n",
    "    if epoch % 2 == 0:\n",
    "        sample = next(iter(val_loader))\n",
    "        with torch.no_grad():\n",
    "            seg_pred = pipeline.segmentation_model(sample['image'].to(pipeline.device))\n",
    "        visualize_results(sample, seg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-llm-zCTe-Zcu-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
