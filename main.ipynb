{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameersharma/Library/Caches/pypoetry/virtualenvs/fine-tuning-llm-zCTe-Zcu-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,  # Add this import\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mandarjoshi/trivia_qa\", \"rc.nocontext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    # Reduce these values\n",
    "    OUTPUT_DIR = \"./results/\"\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    BATCH_SIZE = 2  # Reduced from 8\n",
    "    MAX_LENGTH = 256  # Reduced from 512\n",
    "    GRADIENT_ACCUMULATION_STEPS = 16  # Increase this to compensate for smaller batch size\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_EPOCHS = 3\n",
    "    \n",
    "    # LoRA config\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_DROPOUT = 0.1\n",
    "    \n",
    "    \n",
    "class DatasetProcessor:\n",
    "    \"\"\"Handles dataset loading and processing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_dataset(tokenizer):\n",
    "        print(\"Loading TriviaQA dataset...\")\n",
    "        dataset = load_dataset(\"trivia_qa\", \"rc.nocontext\")\n",
    "        \n",
    "        def format_example(example):\n",
    "            \"\"\"Formats each example into a question-answer pair\"\"\"\n",
    "            # print(example)\n",
    "            # print(example['answer'])\n",
    "            # print(example['answer']['value'])\n",
    "            # print(\"________\")\n",
    "            question = example['question']\n",
    "            answer = example['answer']['value']\n",
    "            return f\"Question: {question}\\nAnswer: {answer}\"\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"\n",
    "            Tokenizes the formatted text and sets up labels for causal language modeling.\n",
    "            The labels should be the same as input_ids for causal LM training.\n",
    "            \"\"\"\n",
    "            # print(examples)\n",
    "            texts = [f\"question: {q}\\n answer: {a['value']}\" for q, a in zip(examples['question'], examples['answer'])]\n",
    "            \n",
    "            # Tokenize inputs\n",
    "            tokenized = tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                max_length=TrainingConfig.MAX_LENGTH,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Set up labels for causal language modeling\n",
    "            tokenized['labels'] = tokenized['input_ids'].clone()\n",
    "            \n",
    "            return tokenized\n",
    "        \n",
    "        print(\"Processing training data...\")\n",
    "        train_dataset = dataset['train'].map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset['train'].column_names\n",
    "        )\n",
    "        \n",
    "        print(\"Processing validation data...\")\n",
    "        val_dataset = dataset['validation'].map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset['validation'].column_names\n",
    "        )\n",
    "        \n",
    "        # Verify the dataset has the required keys\n",
    "        print(\"Dataset keys:\", train_dataset.column_names)  # Should include 'input_ids', 'attention_mask', and 'labels'\n",
    "        \n",
    "        return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSetup:\n",
    "    \"\"\"Handles training configuration and trainer setup\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_training(model, train_dataset, val_dataset, tokenizer):\n",
    "        training_args = transformers.TrainingArguments(\n",
    "            output_dir=TrainingConfig.OUTPUT_DIR,\n",
    "            learning_rate=TrainingConfig.LEARNING_RATE,\n",
    "            num_train_epochs=TrainingConfig.NUM_EPOCHS,\n",
    "            per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "            gradient_accumulation_steps=TrainingConfig.GRADIENT_ACCUMULATION_STEPS,\n",
    "            gradient_checkpointing=True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=500,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=500,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f\"{TrainingConfig.OUTPUT_DIR}/logs\",\n",
    "            logging_steps=100,\n",
    "            # fp16=True,\n",
    "        )\n",
    "        \n",
    "        # Use a custom data collator that handles labels properly\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False  # We're doing causal language modeling, not masked\n",
    "        )\n",
    "        \n",
    "        trainer = transformers.Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        \n",
    "        return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset(dataset, tokenizer):\n",
    "    \"\"\"Utility function to verify the dataset is properly formatted\"\"\"\n",
    "    print(\"\\nVerifying dataset format:\")\n",
    "    example = dataset[0]\n",
    "    print(\"Dataset keys:\", example.keys())\n",
    "    print(\"Input shape:\", len(example['input_ids']))\n",
    "    print(\"Label shape:\", len(example['labels']))\n",
    "    \n",
    "    # Decode an example to verify the format\n",
    "    print(\"\\nExample text:\")\n",
    "    print(\"Input:\", tokenizer.decode(example['input_ids']))\n",
    "    print(\"Labels:\", tokenizer.decode(example['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"Handles loading and preparation of the model and tokenizer\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model_and_tokenizer():\n",
    "        \"\"\"\n",
    "        Loads the Qwen model and tokenizer with appropriate configurations.\n",
    "        - Uses 8-bit quantization to reduce memory usage\n",
    "        - Sets up proper tokenizer configuration\n",
    "        \"\"\"\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            TrainingConfig.MODEL_NAME, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # Ensure proper padding token is set\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "        print(\"Loading model in 8-bit precision...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            TrainingConfig.MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            # quantization_config=quantization_config,  # 8-bit quantization for memory efficiency\n",
    "            device_map=\"auto\",    # Automatically handle model placement on available devices\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAPreparation:\n",
    "    \"\"\"Handles LoRA adapter configuration and setup\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_lora_model(model):\n",
    "        \"\"\"\n",
    "        Configures and applies LoRA adapter to the base model.\n",
    "        Explains which layers are being modified and how many parameters are trainable.\n",
    "        \"\"\"\n",
    "        print(\"Configuring LoRA adapter...\")\n",
    "        lora_config = LoraConfig(\n",
    "            r=TrainingConfig.LORA_R,\n",
    "            lora_alpha=TrainingConfig.LORA_ALPHA,\n",
    "            # Target the attention layers for adaptation\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=TrainingConfig.LORA_DROPOUT,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # Prepare model for k-bit training\n",
    "        print(\"Preparing model for training...\")\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.enable_input_require_grads() \n",
    "        # Apply LoRA adapter\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters information\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_model_setup(model):\n",
    "    \"\"\"Verify model is properly set up for training\"\"\"\n",
    "    print(\"\\nVerifying model setup:\")\n",
    "    \n",
    "    # Check if any parameters require gradients\n",
    "    has_grad_params = any(p.requires_grad for p in model.parameters())\n",
    "    print(f\"Has parameters requiring gradients: {has_grad_params}\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/all_params:.2%} of total)\")\n",
    "    \n",
    "    # Check model device\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return has_grad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model in 8-bit precision...\n",
      "Configuring LoRA adapter...\n",
      "Preparing model for training...\n",
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n",
      "Loading TriviaQA dataset...\n",
      "Processing training data...\n",
      "Processing validation data...\n",
      "Dataset keys: ['input_ids', 'attention_mask', 'labels']\n",
      "\n",
      "Verifying dataset format:\n",
      "Dataset keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Input shape: 256\n",
      "Label shape: 256\n",
      "\n",
      "Example text:\n",
      "Input: question: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
      " answer: Sinclair Lewis<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
      "Labels: question: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
      " answer: Sinclair Lewis<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='12972' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    9/12972 01:01 < 31:30:27, 0.11 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = ModelLoader.load_model_and_tokenizer()\n",
    "model = LoRAPreparation.prepare_lora_model(model)\n",
    "\n",
    "# Process dataset\n",
    "train_dataset, val_dataset = DatasetProcessor.process_dataset(tokenizer)\n",
    "\n",
    "# Verify dataset format\n",
    "verify_dataset(train_dataset, tokenizer)\n",
    "\n",
    "# Setup training\n",
    "trainer = TrainingSetup.setup_training(model, train_dataset, val_dataset, tokenizer)\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.save_model(TrainingConfig.OUTPUT_DIR)\n",
    "model.save_pretrained(f\"{TrainingConfig.OUTPUT_DIR}/adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-llm-zCTe-Zcu-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
